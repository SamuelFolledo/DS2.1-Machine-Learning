{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 Decision Trees\n",
    "- Decision Trees are considered one of the most mature, traditional, algorithms in predictive analytics\n",
    "- They are typically used to solve classification problems through visual and explicit representations of decisions and decision making.\n",
    "- Think of them like a map where you follow each path according to your decision, and each path leads to a new choice to make until you reach the end.\n",
    "- They mimic the way you probably make decisions in your daily life:\n",
    "<img src=\"../static/screenshots/day6-1.png\">\n",
    "\n",
    "## Terminology\n",
    "- **Root**: Our starting point for the tree. Note that a decision tree is drawn upside down since its root is at the top\n",
    "    - Alone Or With Friends is the root in the above example\n",
    "- **Branch**: Also known as an edge, these lead from condition to condition, down to the results\n",
    "    - Sunny or Rainy are branches in the above example\n",
    "- **Condition**: Also known as an internal node, this is the choice that needs to be made in order to figure out which branch to take.\n",
    "    - Weather Outside? is our condiition in the above example\n",
    "- **Leaf**: Also known as a decision, these are the final results that signify the classification of the data. There are no branches coming out of a leaf, only going in to it.\n",
    "    - video games, soccer and movies are all examples of a leaf\n",
    "\n",
    "## Question to the class: Why and when do we need Decision Trees?\n",
    "Shout out or type your answers!\n",
    "\n",
    "### Our answers:\n",
    "- When features are Categorical\n",
    "    - When we can classify data into known groups\n",
    "- When we want to model a set of sequential, hierarchical decisions that lead to some final result. This result is the known group that the data point would be categorized into\n",
    "- When we need to explain the reason for a particular decision\n",
    "- Example use cases:\n",
    "    - Sales and marketing departments might need a complete description of rules that influence the acquisition of a customer before they start their campaign activities\n",
    "    - Product planning (do we build this product or not?)\n",
    "    - Determining someone is a good or bad level of risk\n",
    "    \n",
    "## The root and the leafs for Decision Tree are obtained based on:\n",
    "- Conditional Probability\n",
    "- Entropy\n",
    "- Information Gain\n",
    "\n",
    "## Lens Dataset\n",
    "Let's review the Attribute Information that we know:\n",
    "\n",
    "We have 3 Classes (leaves/results)\n",
    "\n",
    "1. the patient should be fitted with hard contact lenses,\n",
    "2. the patient should be fitted with soft contact lenses,\n",
    "3. the patient should not be fitted with contact lenses.\n",
    "\n",
    "The dataset has 4 Features (conditions):\n",
    "1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic\n",
    "2. spectacle prescription: (1) myope, (2) hypermetrope\n",
    "3. astigmatic: (1) no, (2) yes\n",
    "4. tear production rate: (1) reduced, (2) normal\n",
    "\n",
    "Here is the data used for the Decision Tree:\n",
    "<img src=\"../static/screenshots/day6-2.png\">\n",
    "\n",
    "## Lens Decision Tree Visualized\n",
    "This is ultimately what we want to build using the above dataset\n",
    "<img src=\"../static/screenshots/day6-3.png\">\n",
    "\n",
    "## Decision Trees are based on Entropy\n",
    "### Activity: Calculate the entropy for a coin\n",
    "**Entropy** shows the uncertainy of a random variable. The higher the entropy value, the more unncertain we are. Entropy is displayed as $H(X)$, where $X$ is a random variable\n",
    "\n",
    "The Entropy formula is the summation of probabilities multiplied by the log of probabilities:\n",
    "\n",
    "### Entropy of coin\n",
    "Given p stands for \"probability of\",\n",
    "\n",
    "for outcome in [H,T]:\n",
    "\n",
    "$H(Coin) = \\sum -p(outcome) * log_2(p(outcome)$\n",
    "\n",
    "### Entropy of a fair coin\n",
    "for p(outcome) in [p(H)=0.5, p(T)=0.5]):\n",
    "\n",
    "$H(Coin) = \\sum -p(outcome) * log_2(p(outcome)$\n",
    "\n",
    "### Do the following in pairs:\n",
    "- Create a function entropy that takes an array of probabilities as input, and returns the entropy using the formula above\n",
    "    - numpy's array, log2, and sum functions should be useful here\n",
    "- show that the fair coin has the largest entropy (uncertainty) by trying different values for the probability of heads and tails\n",
    "    - i.e. show that a fair coin [.5, .5] has a larger entropy than a coin with [.9, .1] probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p): #represents the probability of an uncertainty list\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H\n",
    "    \n",
    "p = [.5, .5]\n",
    "# entropy represents uncertainty, a fair coin is the most uncertain case\n",
    "print(entropy(p))\n",
    "\n",
    "p = [.9, .1]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change p (probability of head and tail) and plot the entropy for different values of p\n",
    "<img src=\"../static/screenshots/day6-4.png\" width=400>\n",
    "The fair coin has the highest entropy which means a fair coin has the highest uncertain result when toss a coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of fair dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]\n",
      "2.584962500721156\n"
     ]
    }
   ],
   "source": [
    "p = [1/6]*6\n",
    "print(p)\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How We'll Use Decision Trees Today\n",
    "- You’ll see if a decision tree can give you any insight as to how the eye doctor prescribes contact lenses\n",
    "- You can predict the type of lenses people will use and understand the underlying processes with a decision tree\n",
    "- Predict if a tennis player will play outside based on weather conditions\n",
    "\n",
    "## Quick Review on Conditional Probability\n",
    "We'll be using conditional probability to solve the following activities. Before we do so, let's take 10 minutes to [review conditional probability from DS 1.1](https://github.com/Make-School-Courses/DS-1.1-Data-Analysis/blob/master/Notebooks/Applied_Probability.ipynb)\n",
    "\n",
    "We'll even see the same data set we're about to build a decision tree for!\n",
    "\n",
    "## Let's build a Decision Tree for the Tennis Data\n",
    "The following table shows us the decision making factors used to play tennis outside based on 14 days of data for different weather conditions\n",
    "<img src=\"../static/screenshots/day6-5.png\" width=400>\n",
    "\n",
    "\n",
    "## Activity: Obtain the following quantitites:\n",
    "In groups of 3: Using the tennis dataset, obtain the following quantities:\n",
    "\n",
    "### Entropy for PlayTennis:\n",
    "Obtain the entropy of thePlayTennis (Leaf/Decision) column.\n",
    "\n",
    "### Entropy for PlayTennis conditioned on Weak Wind factor\n",
    "Obtain the entropy of conditional probability p(PlayTennis | Wind = Weak) = [2/8, 6/8]\n",
    "\n",
    "### Entropy for PlayTennis conditioned on Strong Wind factor\n",
    "Obtain the entropy of conditional probability p(PlayTennis | Wind = Strong) = [3/6, 3/6]\n",
    "\n",
    "#### Hints:\n",
    "- p = [9/14, 5/14] which represents the probability that a player plays tennis (9/14 days) or not (5/14 days)\n",
    "- Remember your Entropy function from earlier\n",
    "\n",
    "### Solutions\n",
    "Entropy(Decision) = – (9/14) . log2(9/14) – (5/14) . log2(5/14) = 0.940\n",
    "\n",
    "<img src=\"../static/screenshots/day6-6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook  Temp Humidity    Wind Play\n",
      "1      Sunny   Hot     High    Weak   No\n",
      "2      Sunny   Hot     High  Strong   No\n",
      "3   Overcast   Hot     High    Weak  Yes\n",
      "4       Rain  Mild     High    Weak  Yes\n",
      "5       Rain  Cool   Normal    Weak  Yes\n",
      "6       Rain  Cool   Normal  Strong   No\n",
      "7   Overcast  Cool   Normal  Strong  Yes\n",
      "8      Sunny  Mild     High    Weak   No\n",
      "9      Sunny  Cool   Normal    Weak  Yes\n",
      "10      Rain  Mild   Normal    Weak  Yes\n",
      "11     Sunny  Mild   Normal  Strong  Yes\n",
      "12  Overcast  Mild     High  Strong  Yes\n",
      "13  Overcast   Hot   Normal    Weak  Yes\n",
      "14      Rain  Mild     High  Strong   No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./dataset/tennis.txt', delimiter=\"\\t\", header=None, names=['Outlook', 'Temp', 'Humidity', 'Wind', 'Play'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Yes': 9, 'No': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Play'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p): #represents the probability of an uncertainty list\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H\n",
    "\n",
    "p = [9/14, 5/14]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the entropy of play given the wind is weak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Yes': 6, 'No': 2}\n",
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "print(data[data['Wind'] == 'Weak']['Play'].value_counts().to_dict())\n",
    "\n",
    "p = [6/8, 2/8]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the entropy of play given the wind is strong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No': 3, 'Yes': 3}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(data[data['Wind'] == 'Strong']['Play'].value_counts().to_dict())\n",
    "\n",
    "p = [3/6, 3/6]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "Information Gain **measures how much information a feature gives us about the decision (class)**. This is the main measurement used by a Decision Tree algorithm to construct a Decision Tree!\n",
    "\n",
    "- Decision Trees will always try to maximize information gain\n",
    "- The higher the information gain a feature has, the more likely it is to be tested first\n",
    "    - the feature with the highest information gain will be the first feature in the decision tree, and its branches will lead to the other features\n",
    "\n",
    "## Obtain the Information Gain Between PlayTennis (Decision) and Wind\n",
    "- What is the probability that wind be weak?\n",
    "Hint: Count how many instannces of weak and strong winds we have divided by how many sample we have.\n",
    "\n",
    "p(Wind = Weak) = 8/ 14\n",
    "\n",
    "p(Wind = Strong) = 6/ 14\n",
    "\n",
    "Below are the formulas for finding Information Gain $I(X; Y)$ for a given decision $X$ and feature $Y$, and the Entropy for a decision given a feature\n",
    "\n",
    "<img src=\"../static/screenshots/day6-7.png\" width=500>\n",
    "\n",
    "\n",
    "Given p stands for \"probability of\",\n",
    "\n",
    "for Wind = {Weak, Strong}:\n",
    "\n",
    "- $I(Decision; Wind) = H(Decision) - \\sum p(Wind) * Entropy(Decision | Wind)$\n",
    "\n",
    "We can break this down further:\n",
    "\n",
    "$H(Decision) - \\sum p(Wind) * Entropy(Decision | Wind)$\n",
    "\n",
    "$=$\n",
    "\n",
    "$H(Decision) - (p(Wind = Weak) * H(Decision | Wind = Weak) + p(Wind = Strong) * H(Decision | Wind = Strong)) = 0.048$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04812703040826949"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# information gain between play column and wind column\n",
    "0.9402859586706311 - (8/14 * 0.8112781244591328 + 6/14 * 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other factors on Decision column\n",
    "We have applied similar calculation on the other features (columns)\n",
    "\n",
    "1. Information Gain(Decision, Wind) = 0.048\n",
    "2. Information Gain(Decision, Outlook) = 0.246\n",
    "3. Information Gain(Decision, Temperature) = 0.029\n",
    "4. Information Gain(Decision, Humidity) = 0.151\n",
    "\n",
    "### We can see Outlook and Decision have the highest Gain, so Outlook will be the root for the Decision Tree!\n",
    "### If we keep continuing the calculation of Information Gain between nodes (features), we can build the tree based on the highest Information Gains from feature to feature\n",
    "Example: Information Gain (Outlook, Wind), Information Gain (Outlook, Temperature), Information Gain (Outlook, Humidity), and then finding the information gain after that, and so on and so forth\n",
    "\n",
    "## Build the decision tree with sklearn for tennis dataset\n",
    "<img src=\"../static/screenshots/day6-8.png\" width=600>\n",
    "\n",
    "### For Decision Tree Visualization in Python:\n",
    "Packages that are needed are below. Note that the multiple installs for graphviz are to ensure the executables install correctly to avoid this error:\n",
    "\n",
    "conda install -c anaconda graphviz\n",
    "\n",
    "brew install graphviz\n",
    "\n",
    "conda install -c anaconda pydot\n",
    "\n",
    "conda install -c conda-forge pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper method for entropy\n",
    "def entropy(p): #represents the probability of an uncertainty list\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No': 0.25, 'Yes': 0.75}\n",
      "{'No': 0.5, 'Yes': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# hint: helper function that takes a dataset (df) and one of its features (c1),\n",
    "# decision (c2), and condition of the feature (condition) as input, and outputs\n",
    "# the condiitional probability\n",
    "def conditional_prob(df, c1, c2, condition):\n",
    "    df_new = df[df[c1] == condition][c2]\n",
    "    s = df_new.unique()\n",
    "    population_size = len(df_new)\n",
    "    pr = {}\n",
    "    for i in s:\n",
    "        pr[i] = len(df[(df[c1] == condition) & (df[c2]== i)]) / population_size\n",
    "    \n",
    "    return pr\n",
    "\n",
    "# what are the probabilities of Play  given Wind is Weak?\n",
    "print(conditional_prob(data,'Wind', 'Play', 'Weak'))\n",
    "\n",
    "# what are the probabilities of Play given Wind is Strong?\n",
    "print(conditional_prob(data, 'Wind', 'Play', 'Strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info Gain Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-8b08fa689d13>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-8b08fa689d13>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    def get_entrophy()\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# inputs: dataset (df), a feature from the dataset (feature), and the target (decision)\n",
    "# returns: information gain between feature and decision\n",
    "def info_gain(df, feature, target): #target in tennis dataset would be 'Play' and feature would be Wind, Humidity, Temp, Outlook\n",
    "    #1. obtain entropy of target\n",
    "    target_dict = data[data['Wind'] == 'Strong']['Play'].value_counts().to_dict()\n",
    "    \n",
    "    \n",
    "    #2. obtain different chances (probabilities) of the feature\n",
    "    # example: for Wind, obtain the probabilities of Strong and Weak\n",
    "    \n",
    "    #3. obtain conditional entropy\n",
    "    # obtain the probability of the decision,\n",
    "    # for all possible values of the feature (conditions)\n",
    "    \n",
    "    #4. calculate information gain between feature and target using steps 1, 2, 3. Return step 4\n",
    "    # Given the above metrics, calculate the information gain\n",
    "    # between the feature and the decision using the formula we learned\n",
    "\n",
    "def get_entrophy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milad's Info Gain Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04812703040826949\n",
      "0.15183550136234159\n",
      "0.02922256565895487\n",
      "0.24674981977443933\n"
     ]
    }
   ],
   "source": [
    "# inputs: dataset (df), a feature from the dataset (feature), and the target (decision)\n",
    "# returns: information gain between feature and decision\n",
    "def info_gain(df, feature, decision):\n",
    "    # obtain the entropy of the decision\n",
    "    dict_decision = dict(df[decision].value_counts())\n",
    "    prob_decision = [q for (p,q) in dict_decision.items()]/sum(dict_decision.values())\n",
    "    entropy_decision = entropy(prob_decision)\n",
    "#     print(\"Entropy Decision=\", entropy_decision)\n",
    "    \n",
    "    # obtain the probabilities of the feature\n",
    "    # example: for Wind, obtain the probabilities of Strong and Weak\n",
    "    dict_feature = dict(df[feature].value_counts())\n",
    "    dict_prob_feature = {}\n",
    "    for (p,q) in dict_feature.items():\n",
    "        dict_prob_feature[p] = q/sum(dict_feature.values())\n",
    "#     print(dict_prob_feature)\n",
    "    \n",
    "    # obtain the probability of the decision,\n",
    "    # for all possible values of the feature (conditions)\n",
    "    conditions = df[feature].unique()\n",
    "    dict_ = {}\n",
    "    for condition in conditions:\n",
    "        dict_[condition] = conditional_prob(df, feature, decision, condition)\n",
    "#     print(dict_)\n",
    "    \n",
    "    # Given the above metrics, calculate the information gain\n",
    "    # between the feature and the decision using the formula we learned\n",
    "    S = 0\n",
    "    for (i,j) in dict_.items():\n",
    "#         print(i,j)\n",
    "        prob_condition = list(dict_[i].values())\n",
    "#         print(entropy_condition)\n",
    "        S = S + dict_prob_feature[i]*entropy(prob_condition)\n",
    "#         print(dict_prob_feature[i]*entropy(entropy_condition))\n",
    "    print(entropy_decision - S)\n",
    "    \n",
    "info_gain(data, 'Wind', 'Play')\n",
    "info_gain(data, 'Humidity', 'Play')\n",
    "info_gain(data, 'Temp', 'Play')\n",
    "info_gain(data, 'Outlook', 'Play')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: We can show the information gain between any feature with itself is equal to its entropy:\n",
    "\n",
    "$I(X, X) = H(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35714285714285715, 0.35714285714285715, 0.2857142857142857]\n",
      "1.5774062828523454\n",
      "1.5774062828523454\n",
      "[0.42857142857142855, 0.2857142857142857, 0.2857142857142857]\n",
      "1.5566567074628228\n",
      "1.5566567074628228\n",
      "[0.5, 0.5]\n",
      "1.0\n",
      "1.0\n",
      "[0.5714285714285714, 0.42857142857142855]\n",
      "0.9852281360342515\n",
      "0.9852281360342515\n",
      "[0.6428571428571429, 0.35714285714285715]\n",
      "0.9402859586706311\n",
      "0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "for i in ['Outlook', 'Temp', 'Humidity', 'Wind', 'Play']:\n",
    "    # probability of each feature\n",
    "    p = [m/sum(data[i].value_counts().to_dict().values()) for m in list(data[i].value_counts().to_dict().values())]\n",
    "    print(entropy(p))\n",
    "    info_gain(data, i, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
