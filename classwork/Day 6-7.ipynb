{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 Decision Trees\n",
    "- Decision Trees are considered one of the most mature, traditional, algorithms in predictive analytics\n",
    "- They are typically used to solve classification problems through visual and explicit representations of decisions and decision making.\n",
    "- Think of them like a map where you follow each path according to your decision, and each path leads to a new choice to make until you reach the end.\n",
    "- They mimic the way you probably make decisions in your daily life:\n",
    "<img src=\"../static/screenshots/day6-1.png\">\n",
    "\n",
    "## Terminology\n",
    "- **Root**: Our starting point for the tree. Note that a decision tree is drawn upside down since its root is at the top\n",
    "    - Alone Or With Friends is the root in the above example\n",
    "- **Branch**: Also known as an edge, these lead from condition to condition, down to the results\n",
    "    - Sunny or Rainy are branches in the above example\n",
    "- **Condition**: Also known as an internal node, this is the choice that needs to be made in order to figure out which branch to take.\n",
    "    - Weather Outside? is our condiition in the above example\n",
    "- **Leaf**: Also known as a decision, these are the final results that signify the classification of the data. There are no branches coming out of a leaf, only going in to it.\n",
    "    - video games, soccer and movies are all examples of a leaf\n",
    "\n",
    "## Question to the class: Why and when do we need Decision Trees?\n",
    "Shout out or type your answers!\n",
    "\n",
    "### Our answers:\n",
    "- When features are Categorical\n",
    "    - When we can classify data into known groups\n",
    "- When we want to model a set of sequential, hierarchical decisions that lead to some final result. This result is the known group that the data point would be categorized into\n",
    "- When we need to explain the reason for a particular decision\n",
    "- Example use cases:\n",
    "    - Sales and marketing departments might need a complete description of rules that influence the acquisition of a customer before they start their campaign activities\n",
    "    - Product planning (do we build this product or not?)\n",
    "    - Determining someone is a good or bad level of risk\n",
    "    \n",
    "## The root and the leafs for Decision Tree are obtained based on:\n",
    "- Conditional Probability\n",
    "- Entropy\n",
    "- Information Gain\n",
    "\n",
    "## Lens Dataset\n",
    "Let's review the Attribute Information that we know:\n",
    "\n",
    "We have 3 Classes (leaves/results)\n",
    "\n",
    "1. the patient should be fitted with hard contact lenses,\n",
    "2. the patient should be fitted with soft contact lenses,\n",
    "3. the patient should not be fitted with contact lenses.\n",
    "\n",
    "The dataset has 4 Features (conditions):\n",
    "1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic\n",
    "2. spectacle prescription: (1) myope, (2) hypermetrope\n",
    "3. astigmatic: (1) no, (2) yes\n",
    "4. tear production rate: (1) reduced, (2) normal\n",
    "\n",
    "Here is the data used for the Decision Tree:\n",
    "<img src=\"../static/screenshots/day6-2.png\">\n",
    "\n",
    "## Lens Decision Tree Visualized\n",
    "This is ultimately what we want to build using the above dataset\n",
    "<img src=\"../static/screenshots/day6-3.png\">\n",
    "\n",
    "## Decision Trees are based on Entropy\n",
    "### Activity: Calculate the entropy for a coin\n",
    "**Entropy** shows the uncertainy of a random variable. The higher the entropy value, the more unncertain we are. Entropy is displayed as $H(X)$, where $X$ is a random variable\n",
    "\n",
    "The Entropy formula is the summation of probabilities multiplied by the log of probabilities:\n",
    "\n",
    "### Entropy of coin\n",
    "Given p stands for \"probability of\",\n",
    "\n",
    "for outcome in [H,T]:\n",
    "\n",
    "$H(Coin) = \\sum -p(outcome) * log_2(p(outcome)$\n",
    "\n",
    "### Entropy of a fair coin\n",
    "for p(outcome) in [p(H)=0.5, p(T)=0.5]):\n",
    "\n",
    "$H(Coin) = \\sum -p(outcome) * log_2(p(outcome)$\n",
    "\n",
    "### Do the following in pairs:\n",
    "- Create a function entropy that takes an array of probabilities as input, and returns the entropy using the formula above\n",
    "    - numpy's array, log2, and sum functions should be useful here\n",
    "- show that the fair coin has the largest entropy (uncertainty) by trying different values for the probability of heads and tails\n",
    "    - i.e. show that a fair coin [.5, .5] has a larger entropy than a coin with [.9, .1] probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p): #represents the probability of an uncertainty list\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H\n",
    "    \n",
    "p = [.5, .5]\n",
    "# entropy represents uncertainty, a fair coin is the most uncertain case\n",
    "print(entropy(p))\n",
    "\n",
    "p = [.9, .1]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change p (probability of head and tail) and plot the entropy for different values of p\n",
    "<img src=\"../static/screenshots/day6-4.png\" width=400>\n",
    "The fair coin has the highest entropy which means a fair coin has the highest uncertain result when toss a coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of fair dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]\n",
      "2.584962500721156\n"
     ]
    }
   ],
   "source": [
    "p = [1/6]*6\n",
    "print(p)\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How We'll Use Decision Trees Today\n",
    "- You’ll see if a decision tree can give you any insight as to how the eye doctor prescribes contact lenses\n",
    "- You can predict the type of lenses people will use and understand the underlying processes with a decision tree\n",
    "- Predict if a tennis player will play outside based on weather conditions\n",
    "\n",
    "## Quick Review on Conditional Probability\n",
    "We'll be using conditional probability to solve the following activities. Before we do so, let's take 10 minutes to [review conditional probability from DS 1.1](https://github.com/Make-School-Courses/DS-1.1-Data-Analysis/blob/master/Notebooks/Applied_Probability.ipynb)\n",
    "\n",
    "We'll even see the same data set we're about to build a decision tree for!\n",
    "\n",
    "## Let's build a Decision Tree for the Tennis Data\n",
    "The following table shows us the decision making factors used to play tennis outside based on 14 days of data for different weather conditions\n",
    "<img src=\"../static/screenshots/day6-5.png\" width=400>\n",
    "\n",
    "\n",
    "## Activity: Obtain the following quantitites:\n",
    "In groups of 3: Using the tennis dataset, obtain the following quantities:\n",
    "\n",
    "### Entropy for PlayTennis:\n",
    "Obtain the entropy of thePlayTennis (Leaf/Decision) column.\n",
    "\n",
    "### Entropy for PlayTennis conditioned on Weak Wind factor\n",
    "Obtain the entropy of conditional probability p(PlayTennis | Wind = Weak) = [2/8, 6/8]\n",
    "\n",
    "### Entropy for PlayTennis conditioned on Strong Wind factor\n",
    "Obtain the entropy of conditional probability p(PlayTennis | Wind = Strong) = [3/6, 3/6]\n",
    "\n",
    "#### Hints:\n",
    "- p = [9/14, 5/14] which represents the probability that a player plays tennis (9/14 days) or not (5/14 days)\n",
    "- Remember your Entropy function from earlier\n",
    "\n",
    "### Solutions\n",
    "Entropy(Decision) = – (9/14) . log2(9/14) – (5/14) . log2(5/14) = 0.940\n",
    "\n",
    "<img src=\"../static/screenshots/day6-6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook  Temp Humidity    Wind Play\n",
      "1      Sunny   Hot     High    Weak   No\n",
      "2      Sunny   Hot     High  Strong   No\n",
      "3   Overcast   Hot     High    Weak  Yes\n",
      "4       Rain  Mild     High    Weak  Yes\n",
      "5       Rain  Cool   Normal    Weak  Yes\n",
      "6       Rain  Cool   Normal  Strong   No\n",
      "7   Overcast  Cool   Normal  Strong  Yes\n",
      "8      Sunny  Mild     High    Weak   No\n",
      "9      Sunny  Cool   Normal    Weak  Yes\n",
      "10      Rain  Mild   Normal    Weak  Yes\n",
      "11     Sunny  Mild   Normal  Strong  Yes\n",
      "12  Overcast  Mild     High  Strong  Yes\n",
      "13  Overcast   Hot   Normal    Weak  Yes\n",
      "14      Rain  Mild     High  Strong   No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./dataset/tennis.txt', delimiter=\"\\t\", header=None, names=['Outlook', 'Temp', 'Humidity', 'Wind', 'Play'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Yes': 9, 'No': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Play'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p): #represents the probability of an uncertainty list\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H\n",
    "\n",
    "p = [9/14, 5/14]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the entropy of play given the wind is weak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Yes': 6, 'No': 2}\n",
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "print(data[data['Wind'] == 'Weak']['Play'].value_counts().to_dict())\n",
    "\n",
    "p = [6/8, 2/8]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the entropy of play given the wind is strong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No': 3, 'Yes': 3}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(data[data['Wind'] == 'Strong']['Play'].value_counts().to_dict())\n",
    "\n",
    "p = [3/6, 3/6]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "Information Gain **measures how much information a feature gives us about the decision (class)**. This is the main measurement used by a Decision Tree algorithm to construct a Decision Tree!\n",
    "\n",
    "- Decision Trees will always try to maximize information gain\n",
    "- The higher the information gain a feature has, the more likely it is to be tested first\n",
    "    - the feature with the highest information gain will be the first feature in the decision tree, and its branches will lead to the other features\n",
    "\n",
    "## Obtain the Information Gain Between PlayTennis (Decision) and Wind\n",
    "- What is the probability that wind be weak?\n",
    "Hint: Count how many instannces of weak and strong winds we have divided by how many sample we have.\n",
    "\n",
    "p(Wind = Weak) = 8/ 14\n",
    "\n",
    "p(Wind = Strong) = 6/ 14\n",
    "\n",
    "Below are the formulas for finding Information Gain $I(X; Y)$ for a given decision $X$ and feature $Y$, and the Entropy for a decision given a feature\n",
    "\n",
    "<img src=\"../static/screenshots/day6-7.png\" width=500>\n",
    "\n",
    "\n",
    "Given p stands for \"probability of\",\n",
    "\n",
    "for Wind = {Weak, Strong}:\n",
    "\n",
    "- $I(Decision; Wind) = H(Decision) - \\sum p(Wind) * Entropy(Decision | Wind)$\n",
    "\n",
    "We can break this down further:\n",
    "\n",
    "$H(Decision) - \\sum p(Wind) * Entropy(Decision | Wind)$\n",
    "\n",
    "$=$\n",
    "\n",
    "$H(Decision) - (p(Wind = Weak) * H(Decision | Wind = Weak) + p(Wind = Strong) * H(Decision | Wind = Strong)) = 0.048$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04812703040826949"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# information gain between play column and wind column\n",
    "0.9402859586706311 - (8/14 * 0.8112781244591328 + 6/14 * 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other factors on Decision column\n",
    "We have applied similar calculation on the other features (columns)\n",
    "\n",
    "1. Information Gain(Decision, Wind) = 0.048\n",
    "2. Information Gain(Decision, Outlook) = 0.246\n",
    "3. Information Gain(Decision, Temperature) = 0.029\n",
    "4. Information Gain(Decision, Humidity) = 0.151\n",
    "\n",
    "### We can see Outlook and Decision have the highest Gain, so Outlook will be the root for the Decision Tree!\n",
    "### If we keep continuing the calculation of Information Gain between nodes (features), we can build the tree based on the highest Information Gains from feature to feature\n",
    "Example: Information Gain (Outlook, Wind), Information Gain (Outlook, Temperature), Information Gain (Outlook, Humidity), and then finding the information gain after that, and so on and so forth\n",
    "\n",
    "## Build the decision tree with sklearn for tennis dataset\n",
    "<img src=\"../static/screenshots/day6-8.png\" width=600>\n",
    "\n",
    "### For Decision Tree Visualization in Python:\n",
    "Packages that are needed are below. Note that the multiple installs for graphviz are to ensure the executables install correctly to avoid this error:\n",
    "\n",
    "conda install -c anaconda graphviz\n",
    "\n",
    "brew install graphviz\n",
    "\n",
    "conda install -c anaconda pydot\n",
    "\n",
    "conda install -c conda-forge pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper method for entropy\n",
    "def entropy(p): #represents the probability of an uncertainty list\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No': 0.25, 'Yes': 0.75}\n",
      "{'No': 0.5, 'Yes': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# hint: helper function that takes a dataset (df) and one of its features (c1),\n",
    "# decision (c2), and condition of the feature (condition) as input, and outputs\n",
    "# the condiitional probability\n",
    "def conditional_prob(df, c1, c2, condition):\n",
    "    df_new = df[df[c1] == condition][c2]\n",
    "    s = df_new.unique()\n",
    "    population_size = len(df_new)\n",
    "    pr = {}\n",
    "    for i in s:\n",
    "        pr[i] = len(df[(df[c1] == condition) & (df[c2]== i)]) / population_size\n",
    "    \n",
    "    return pr\n",
    "\n",
    "# what are the probabilities of Play  given Wind is Weak?\n",
    "print(conditional_prob(data,'Wind', 'Play', 'Weak'))\n",
    "\n",
    "# what are the probabilities of Play given Wind is Strong?\n",
    "print(conditional_prob(data, 'Wind', 'Play', 'Strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info Gain Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-8b08fa689d13>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-8b08fa689d13>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    def get_entrophy()\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# inputs: dataset (df), a feature from the dataset (feature), and the target (decision)\n",
    "# returns: information gain between feature and decision\n",
    "def info_gain(df, feature, target): #target in tennis dataset would be 'Play' and feature would be Wind, Humidity, Temp, Outlook\n",
    "    #1. obtain entropy of target\n",
    "    target_dict = data[data['Wind'] == 'Strong']['Play'].value_counts().to_dict()\n",
    "    \n",
    "    \n",
    "    #2. obtain different chances (probabilities) of the feature\n",
    "    # example: for Wind, obtain the probabilities of Strong and Weak\n",
    "    \n",
    "    #3. obtain conditional entropy\n",
    "    # obtain the probability of the decision,\n",
    "    # for all possible values of the feature (conditions)\n",
    "    \n",
    "    #4. calculate information gain between feature and target using steps 1, 2, 3. Return step 4\n",
    "    # Given the above metrics, calculate the information gain\n",
    "    # between the feature and the decision using the formula we learned\n",
    "\n",
    "def get_entrophy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milad's Info Gain Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04812703040826949\n",
      "0.15183550136234159\n",
      "0.02922256565895487\n",
      "0.24674981977443933\n"
     ]
    }
   ],
   "source": [
    "# inputs: dataset (df), a feature from the dataset (feature), and the target (decision)\n",
    "# returns: information gain between feature and decision\n",
    "def info_gain(df, feature, decision):\n",
    "    # obtain the entropy of the decision\n",
    "    dict_decision = dict(df[decision].value_counts())\n",
    "    prob_decision = [q for (p,q) in dict_decision.items()]/sum(dict_decision.values())\n",
    "    entropy_decision = entropy(prob_decision)\n",
    "#     print(\"Entropy Decision=\", entropy_decision)\n",
    "    \n",
    "    # obtain the probabilities of the feature\n",
    "    # example: for Wind, obtain the probabilities of Strong and Weak\n",
    "    dict_feature = dict(df[feature].value_counts())\n",
    "    dict_prob_feature = {}\n",
    "    for (p,q) in dict_feature.items():\n",
    "        dict_prob_feature[p] = q/sum(dict_feature.values())\n",
    "#     print(dict_prob_feature)\n",
    "    \n",
    "    # obtain the probability of the decision,\n",
    "    # for all possible values of the feature (conditions)\n",
    "    conditions = df[feature].unique()\n",
    "    dict_ = {}\n",
    "    for condition in conditions:\n",
    "        dict_[condition] = conditional_prob(df, feature, decision, condition)\n",
    "#     print(dict_)\n",
    "    \n",
    "    # Given the above metrics, calculate the information gain\n",
    "    # between the feature and the decision using the formula we learned\n",
    "    S = 0\n",
    "    for (i,j) in dict_.items():\n",
    "#         print(i,j)\n",
    "        prob_condition = list(dict_[i].values())\n",
    "#         print(entropy_condition)\n",
    "        S = S + dict_prob_feature[i]*entropy(prob_condition)\n",
    "#         print(dict_prob_feature[i]*entropy(entropy_condition))\n",
    "    print(entropy_decision - S)\n",
    "    \n",
    "info_gain(data, 'Wind', 'Play')\n",
    "info_gain(data, 'Humidity', 'Play')\n",
    "info_gain(data, 'Temp', 'Play')\n",
    "info_gain(data, 'Outlook', 'Play')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: We can show the information gain between any feature with itself is equal to its entropy:\n",
    "\n",
    "$I(X, X) = H(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35714285714285715, 0.35714285714285715, 0.2857142857142857]\n",
      "1.5774062828523454\n",
      "1.5774062828523454\n",
      "[0.42857142857142855, 0.2857142857142857, 0.2857142857142857]\n",
      "1.5566567074628228\n",
      "1.5566567074628228\n",
      "[0.5, 0.5]\n",
      "1.0\n",
      "1.0\n",
      "[0.5714285714285714, 0.42857142857142855]\n",
      "0.9852281360342515\n",
      "0.9852281360342515\n",
      "[0.6428571428571429, 0.35714285714285715]\n",
      "0.9402859586706311\n",
      "0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "for i in ['Outlook', 'Temp', 'Humidity', 'Wind', 'Play']:\n",
    "    # probability of each feature\n",
    "    p = [m/sum(data[i].value_counts().to_dict().values()) for m in list(data[i].value_counts().to_dict().values())]\n",
    "    print(entropy(p))\n",
    "    info_gain(data, i, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: Principal Component Analysis\n",
    "\n",
    "## Principel Component Analysis (PCA)\n",
    "- PCA is a well-known algorithm for **Dimensionality Reduction**\n",
    "- PCA:\n",
    "    - Reduces the number of features while keeping the features information\n",
    "    - Removes correlations among features\n",
    "    - Emphasizes variation of strong features, making the data easier to visualize\n",
    "\n",
    "## Check in for Pre-Watching of PCA:\n",
    "Going forward, it is assumed that you have already watched the following videos:\n",
    "- What is PCA?: https://www.youtube.com/watch?v=HMOI_lkzW08\n",
    "- What is a covariance matrix?: https://www.youtube.com/watch?v=0GzMcUy7ZI0\n",
    "- How to multiply a matrix with a vector?: https://www.youtube.com/watch?v=Awcj447pYuk\n",
    "\n",
    "Are there any questions about these videos?\n",
    "\n",
    "## Review matrix multiplication\n",
    "- Matrix `A = np.array([[2, 0], [1, 5]])` and vector `v = np.array([3, 4])` are given.\n",
    "- Question: What is the multiplication of A by v?\n",
    "\n",
    "Solve using the following methods:\n",
    "- Compute it by hand\n",
    "- Write a Python function to compute it (Hint: use the following function form numpy: `np.dot(A, v))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 23]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[2, 0], [1, 5]])\n",
    "v = np.array([3, 4])\n",
    "\n",
    "print(np.dot(A, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EigenValue and Eigenvector of matrix\n",
    "A martix's eigenvalues and eigenvectors are what we **will use for the scalar value a and vector v** respectively.\n",
    "- **Eigenvector (v)** is a **vector whose direction remains unchanged** when a linear transformation is applied to it. They represent the rotation matrix\n",
    "- **Eigenvalues (a)** represents the **scalar value that is used such that when multiplied by v, gives the same value as Av**\n",
    "\n",
    "For given matrix A, we want to obtain a vector v and a scalar value a such that:\n",
    "\n",
    "`Av = av`\n",
    "\n",
    "### Write a Python function to obtain vector v and scalar a for a given matrix A\n",
    "You will use the same matrix A that we used above.\n",
    "\n",
    "**hints**:\n",
    "\n",
    "1. Before we find the vector and scalar, we need the eigenvalue and eigenvector of A. Given the same matrix A we used above, see [how numpy's linalg.eig method](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) could help you solve this\n",
    "2. To check your answer, multiply A by one of its vectors, and then multiply a by the same vector, and see if you get the same outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen value= [5. 2.]\n",
      "Eigen vector= [[ 0.          0.9486833 ]\n",
      " [ 1.         -0.31622777]]\n",
      "\n",
      "A = 5, 2 and v = 0, 1\n"
     ]
    }
   ],
   "source": [
    "#1.\n",
    "eig_value, eig_vector = np.linalg.eig(A)\n",
    "\n",
    "print(\"Eigen value=\", eig_value)\n",
    "print(\"Eigen vector=\", eig_vector)\n",
    "print(\"\\nA = 5, 2 and v = 0, 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Av = av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 5.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.\n",
    "# A = np.array([[2, 0], [1, 5]])\n",
    "# v = np.array([0, 1])\n",
    "# print(A[1][1] * v)\n",
    "# print(np.dot(A,v))\n",
    "\n",
    "# multiply A with its first eigen-vector\n",
    "np.dot(A, eig_vector[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 5.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply the one eigen-value of A with its associated eigen-vector\n",
    "eig_value[0] * eig_vector[:, 0] # 5 * [0., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8973666 , -0.63245553])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly, multiply A with its second eigen-vector\n",
    "np.dot(A, eig_vector[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8973666 , -0.63245553])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply the other eigen-value of A with its associated eigen-vector\n",
    "eig_value[1] * eig_vector[:, 1] # 2.0 * [0.9486833 , -0.31622777]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Are the countries in great UK different in terms of food?\n",
    "- In the table is the average consumption of 17 types of food in grams per person per week for every country in the UK\n",
    "- We want to visually represent the diffrence among UK countries based on the food they eat, but this can be difficult when there's 17 types of food (dimensions) to consider. The graph would be incredibly hard to read!\n",
    "- This is where PCA comes in to play: through PCA, we can consolidate the 17 types into what we call principle components.\n",
    "- **Principle Components** allow us to **take an arbitrary number of data points (let's say 17) and consolidate them into a single (x, y)** datapoint for a given feature.\n",
    "\n",
    "<img src=\"../static/screenshots/day7-1.png\" width=800>\n",
    "\n",
    "## Question:\n",
    "Which country is different from the the others? Any idea or reasoning?\n",
    "Do it together: Write a function that obtains the principle components from 17 types of food in UK\n",
    "Get in groups of 3 for this activity\n",
    "\n",
    "#### Setup:\n",
    "- Download the dataset we will use for this activity: [pca_uk](https://render.githubusercontent.com/view/Datasets/pca_uk.xlsx)\n",
    "- Run the following in your terminal:\n",
    "    - `conda install -c anaconda xlrd`\n",
    "    - `pip3 install xlrd`\n",
    "    \n",
    "We will use two principle components as an example to see them visually, but we can pick 3 or more principle components as well\n",
    "\n",
    "#### Outline to follow:\n",
    "- use pandas to read in the excel spreadsheet (research how pandas can read an excel file)\n",
    "- build a matrix of the feature values, not including the text labels\n",
    "- calculate the PCA. This [sklearn module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) may be useful\n",
    "- Obtain the principle components. This can be done by [applying the dimensionality reduction onto our matrix](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df=     England  N Ireland  Scotland  Wales            Features\n",
      "0       375        135       458    475    Alcoholic drinks\n",
      "1        57         47        53     73           Beverages\n",
      "2       245        267       242    227        Carcase meat\n",
      "3      1472       1494      1462   1582             Cereals\n",
      "4       105         66       103    103              Cheese\n",
      "5        54         41        62     64       Confectionery\n",
      "6       193        209       184    235       Fats and oils\n",
      "7       147         93       122    160                Fish\n",
      "8      1102        674       957   1137         Fresh fruit\n",
      "9       720       1033       566    874      Fresh potatoes\n",
      "10      253        143       171    265           Fresh Veg\n",
      "11      685        586       750    803          Other meat\n",
      "12      488        355       418    570           Other Veg\n",
      "13      198        187       220    203  Processed potatoes\n",
      "14      360        334       337    365       Processed Veg\n",
      "15     1374       1506      1572   1256         Soft drinks\n",
      "16      156        139       147    175              Sugars\n",
      "df features= ['Alcoholic drinks' 'Beverages' 'Carcase meat' 'Cereals' 'Cheese'\n",
      " 'Confectionery' 'Fats and oils' 'Fish' 'Fresh fruit' 'Fresh potatoes'\n",
      " 'Fresh Veg' 'Other meat' 'Other Veg' 'Processed potatoes' 'Processed Veg'\n",
      " 'Soft drinks' 'Sugars']\n",
      "\n",
      "X=\n",
      "[[ 375   57  245 1472  105   54  193  147 1102  720  253  685  488  198\n",
      "   360 1374  156]\n",
      " [ 135   47  267 1494   66   41  209   93  674 1033  143  586  355  187\n",
      "   334 1506  139]\n",
      " [ 458   53  242 1462  103   62  184  122  957  566  171  750  418  220\n",
      "   337 1572  147]\n",
      " [ 475   73  227 1582  103   64  235  160 1137  874  265  803  570  203\n",
      "   365 1256  175]]\n",
      "\n",
      "X_r=\n",
      "[[-144.99315218   -2.53299944]\n",
      " [ 477.39163882  -58.90186182]\n",
      " [ -91.869339    286.08178613]\n",
      " [-240.52914764 -224.64692488]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use pandas to read in the excel spreadsheet\n",
    "df = pd.read_excel('./dataset/pca_uk.xlsx')\n",
    "print(\"df=\", df)\n",
    "print(\"df features=\", df['Features'].values)\n",
    "\n",
    "# build a matrix of the feature values, not including the text labels\n",
    "X = np.array([df[i].values for i in df.columns if i != 'Features'])\n",
    "\n",
    "print(\"\\nX=\")\n",
    "print(X)\n",
    "\n",
    "# calculate the PCA\n",
    "pca = PCA(n_components=2) # apply 2 components only\n",
    "\n",
    "# Find the principle components of 17 features\n",
    "X_r = pca.fit_transform(X)\n",
    "\n",
    "print(\"\\nX_r=\")\n",
    "print(X_r)\n",
    "# Will output\n",
    "# [[-144.99315218   -2.53299944] # PCA 1 and 2 of England\n",
    "#  [ 477.39163882  -58.90186182] # PCA 1 and 2 of N Ireland\n",
    "#  [ -91.869339    286.08178613] # PCA 1 and 2 of Scottland\n",
    "#  [-240.52914764 -224.64692488]] # PCA 1 and 2 fo Wales\n",
    "#  However we cannot assign any meaningful names/label for these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets visualize the principle components\n",
    "\n",
    "for feature, (plot_x,plot_y) in enumerate(zip(X_r[:, 0], X_r[:, 1])):\n",
    "    plt.scatter(plot_x, plot_y)\n",
    "    plt.text(plot_x+0.3, plot_y+0.3, df.columns[:-1][feature])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more **PCA and number of components**, read https://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: Ireland is different from other three countries in UK\n",
    "Why is Ireland such an outlier?\n",
    "\n",
    "## How much of the information in the original dataset is preserved in the principle components?\n",
    "Hint: use [pca.explained_variance_ratio_](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_r=\n",
      " [[-144.99315218   -2.53299944]\n",
      " [ 477.39163882  -58.90186182]\n",
      " [ -91.869339    286.08178613]\n",
      " [-240.52914764 -224.64692488]] \n",
      "\n",
      "explained_variance_= [105073.34576714  45261.62487597]\n",
      "explained_variance_ratio_= [0.67444346 0.29052475]\n",
      "explained_variance_ratio_.cumsum()= [0.67444346 0.96496821]\n"
     ]
    }
   ],
   "source": [
    "# PCA computation by sklearn\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit_transform(X)\n",
    "print(\"X_r=\\n\", X_r, \"\\n\")\n",
    "print(\"explained_variance_=\", pca.explained_variance_)\n",
    "print(\"explained_variance_ratio_=\", pca.explained_variance_ratio_) #outputs [0.67444346 0.29052475]. \n",
    "print(\"explained_variance_ratio_.cumsum()=\", pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "`pca.explained_variance_ratio_` = [0.67444346 0.29052475]\n",
    "\n",
    "- Meaning the first principle component preserves 67% of the information and the second principle component preserve 29% which totals 96% of the information in X is preserved from two components.\n",
    "\n",
    "So we always should define our threshold level (for example 95%) then if the `explained_variance_ratio` is below we increase the number of assigned components. With our dataset, with 2 components, 96% of information in X is in X_r (X_reduced)\n",
    "\n",
    "\n",
    "## How to calculate the correlation of the principle components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of PCA Component:\n",
      "(0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "print('Correlation of PCA Component:')\n",
    "print(scipy.stats.pearsonr(X_r[:, 0], X_r[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity (Reminder): Reverse the elemnts of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 2, 1]\n",
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "def reverse_list(list):\n",
    "    reversed_list = []\n",
    "    list_length = len(list)\n",
    "    for i in range(list_length):\n",
    "        current = list[list_length - (i + 1)]\n",
    "        reversed_list.append(current)\n",
    "    return reversed_list\n",
    "\n",
    "list_to_reverse = [1,2,3,4,5]\n",
    "print(reverse_list(list_to_reverse))\n",
    "\n",
    "# OR\n",
    "print(list_to_reverse[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets write our own function to obtain principle components\n",
    "### Activity: PCA Steps\n",
    "**In groups of 3**: Follow the steps here and write a function that computes the principle components for a dataset similar to the one we watched on YouTube: https://www.youtube.com/watch?v=0GzMcUy7ZI0\n",
    "\n",
    "### Steps:\n",
    "1. Use the following matrix: `X = np.array([[1, 1, 1], [1, 2, 1], [1, 3, 2], [1, 4, 3]])` #4 rows with 3 features, should be reduced to 4 rows and 2 columns\n",
    "2. Subtract the column mean from the feature matrix -> this new matrix will be our centered matrix\n",
    "3. Calculate the covariance of the centered matrix (check out numpy's resources to see if there's a function that can do this for you...) --> this new matrix will be our covariance matrix.\n",
    "4. Calculate the eigenvalue and eigenvector of the covariance matrix. Remember how we did this in a previous activity!\n",
    "5. Sort the eigevalues so that they are in descending order, and then find the top N (for example, 2) eigenvectors\n",
    "6. Dot multiply the centered matrix with the top N eigenvectors of the covariance matrix\n",
    "7. Compare the result of custom function with PCA in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mysolution - unfinished and wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.32212166e+00 9.45450057e-02 7.89185006e-17 3.99762102e-18]\n",
      "\n",
      " [[-0.63281645  0.59122191 -0.03786412  0.37621558]\n",
      " [-0.27179185 -0.71376644  0.36921189  0.64511685]\n",
      " [ 0.21093882 -0.19707397 -0.85201612 -0.16158696]\n",
      " [ 0.69366948  0.3196185   0.36921189  0.64511685]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "\n",
    "def calculate_PCA(array, comp_num):\n",
    "    #2.\n",
    "    col_mean = array.mean(axis=0) #array of mean of each columns\n",
    "#     print(col_mean)\n",
    "    center_matrix = array - col_mean #Subtract the column mean from the feature matrix\n",
    "#     print(center_matrix)\n",
    "    #3.\n",
    "    cov = np.cov(center_matrix)\n",
    "#     print(cov)\n",
    "    #4. eigenvalue and eigenvector of the covariance matrix\n",
    "    eig_value, eig_vector = np.linalg.eig(cov)\n",
    "    #5. sort in descending order\n",
    "    eigvalues_reversed = np.sort(eig_value)[::-1]\n",
    "    print(eigvalues_reversed)\n",
    "    # find the top eigenvectors\n",
    "    print(\"\\n\", eig_vector)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "X = np.array([[1, 1, 1], [1, 2, 1], [1, 3, 2], [1, 4, 3]])\n",
    "calculate_PCA(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milad's PCA solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.65392786 -0.2775295 ]\n",
      " [-0.84584087  0.31153366]\n",
      " [ 0.55130929  0.09250983]\n",
      " [ 1.94845944 -0.126514  ]]\n",
      "[2.5171201  0.06621324]\n",
      "[0.97436907 0.02563093]\n",
      "[0.97436907 1.        ]\n",
      "Correlation of PCA Component:\n",
      "(3.885780586188048e-16, 0.9999999999999996)\n",
      "[[0.         0.         0.        ]\n",
      " [0.         1.66666667 1.16666667]\n",
      " [0.         1.16666667 0.91666667]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.65392786, -0.2775295 ],\n",
       "       [ 0.84584087,  0.31153366],\n",
       "       [-0.55130929,  0.09250983],\n",
       "       [-1.94845944, -0.126514  ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "\n",
    "# PCA computation by sklearn\n",
    "\n",
    "X = np.array([[1, 1, 1], [1, 2, 1], [1, 3, 2], [1, 4, 3]])\n",
    "# print(X)\n",
    "pca = PCA(n_components=2) \n",
    "X_r = pca.fit_transform(X)\n",
    "print(X_r)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "print('Correlation of PCA Component:')\n",
    "print(scipy.stats.pearsonr(X_r[:, 0], X_r[:, 1]))\n",
    "\n",
    "\n",
    "# Our function to compare \n",
    "def PCA_calculation(data, n_comp=2):\n",
    "    M = np.mean(data, axis=0)\n",
    "    # center columns by subtracting column means\n",
    "    C =  data - M\n",
    "    # calculate covariance matrix of centered matrix\n",
    "    V = np.cov(C.T)\n",
    "    print(V)\n",
    "    # eigen decomposition of covariance matrix\n",
    "    eig_value, eig_vector = np.linalg.eig(V)\n",
    "    # sort eigenvalue in decreasing order\n",
    "    idx = np.argsort(eig_value)[::-1] #sort and get reverse (returns an array of index)\n",
    "    idx_n_comp = idx[:n_comp] # get the index that is top 2\n",
    "    # eigenvectors according to top n_comp largest\n",
    "    eig_vector = eig_vector[:, idx_n_comp] # get the eig_vector that from the top 2 index\n",
    "    P = np.dot(C, eig_vector)\n",
    "    return P\n",
    "\n",
    "\n",
    "PCA_calculation(X, 2) # is what is happening in these 2 lines pca = PCA(n_components=2) AND X_r = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Is PCA Supervised or Unsupervised?\n",
    "- Did we use any label to do dimensionality reduction?\n",
    "\n",
    "## Activity: Apply Principle to Boston housing features and then train the linear regression model\n",
    "- Basically, we remove correlation among features with PCA\n",
    "- We do not need to do feature data scaling (normalization) when we do PCA for features, because\n",
    "- Report the R-squared and MSE for a system with PCA+Linear Regression\n",
    "\n",
    "## Summary:\n",
    "- PCA is a **mathematical technique to reduce redundancy in data, and is an algorithm for Dimensionality Reduction\n",
    "- PCA emphasizes variation and strong patterns, making the data easier to visualize\n",
    "- We use eigenvectors and eigenvalues to obtain the principle component (our new features) in lower dimension\n",
    "\n",
    "Resources:\n",
    "http://setosa.io/ev/principal-component-analysis/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
